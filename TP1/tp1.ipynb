{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3ee81e-8c19-4bbe-adde-543dc38de9fe",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ae43cf05-9951-43c0-a836-308fbb90f1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 0.24873167822311673\n",
      "E 0.7512683217768833\n",
      "\n",
      "I 0.6186558869646712\n",
      "E 0.3813441130353287\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def train(self, df, alpha=1):\n",
    "        grouped_classes = df.groupby('Nacionalidad')\n",
    "        self.classes = df['Nacionalidad'].unique()\n",
    "        self.attributes = df.columns[0: -1]\n",
    "        self.class_probabilities = {}\n",
    "        self.attribute_probabilities = {}\n",
    "        total_len = len(df)\n",
    "\n",
    "        for _class, grouped in grouped_classes:\n",
    "            data_len = len(grouped)\n",
    "            self.class_probabilities[_class] = data_len/ total_len\n",
    "            self.attribute_probabilities[_class] = {}\n",
    "            for attribute in df.columns[0:-1]:    # Skipping last column (Nacionalidad)\n",
    "                self.attribute_probabilities[_class][attribute] = (grouped[attribute].sum() + alpha) / (data_len + alpha * len(self.attributes))\n",
    "    \n",
    "    def print_probabilities(self, values):\n",
    "        prediction = {}\n",
    "        for _class in self.classes:\n",
    "            probability = self.class_probabilities[_class]\n",
    "            for i,value in enumerate(values):\n",
    "                if value == 1:\n",
    "                    probability *= self.attribute_probabilities[_class][self.attributes[i]]\n",
    "                else: \n",
    "                    probability *= 1 - self.attribute_probabilities[_class][self.attributes[i]] # 1 - P(A) = P(not A)\n",
    "            prediction[_class] = probability\n",
    "        for _class, value in prediction.items():\n",
    "            print(_class, value / sum(prediction.values()))  # divide the probability by the sum of all probabilities to get the normalized probability\n",
    "\n",
    "\n",
    "file = 'inputs/PreferenciasBritanicos.xlsx'\n",
    "brits_df = pd.read_excel(file)\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(brits_df)\n",
    "# 1.B\n",
    "classifier.print_probabilities([1, 0, 1, 1, 1])\n",
    "print()\n",
    "# 1.C\n",
    "classifier.print_probabilities([0, 1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11587fba",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "## Paso 1: preprocesamiento del dataset\n",
    "Primero necesitamos hacer un preprocesamiento de los datos. Para esto decidimos reducir el dataset a 4 categorías, preferencialmente que no guarden demasiada correlación entre ellas en cuanto a las palabras utilizadas y asegurarnos que el modelo logre diferenciarlas correctamente sin tener que lidiar con las otras categorías. Una vez que el objetivo sea cumplido agregaremos devuelta las categorías filtradas para utilizar el dataset completo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48324d",
   "metadata": {},
   "source": [
    "Para el preprocesamiento de los titulos utilizaremos tecnicas utilizadas en NLP y las aplicaremos a los titulos, procedemos a la tokenización de las palabras en los titulos,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "96e569ea-e095-4385-a562-4d6c74c5e26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeliv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package snowball_data to\n",
      "[nltk_data]     C:\\Users\\yeliv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package snowball_data is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salud\n",
      "Destacadas\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 219\u001b[0m\n\u001b[0;32m    217\u001b[0m classifier\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m    218\u001b[0m classifier\u001b[39m.\u001b[39mpredict_title(\u001b[39m'\u001b[39m\u001b[39mDiabetes : este miércoles se realiza la jornada de prevención\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 219\u001b[0m classifier\u001b[39m.\u001b[39;49mcalculate_confusion_matrix()\n\u001b[0;32m    220\u001b[0m classifier\u001b[39m.\u001b[39mprint_confusion_matrix()\n\u001b[0;32m    221\u001b[0m \u001b[39m# roc_matrix = classifier.calculate_confusion_matrix_ROC('Nacional')\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39m# print(\"accuracy: \",classifier.calculate_accuracy(roc_matrix))\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39m# print(\"true_positives: \",classifier.calculate_true_positives('Nacional',roc_matrix))\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39m# print(\"precision: \",classifier.calculate_precision('Nacional', roc_matrix))\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39m# print(\"f1-score: \",classifier.calculate_f1_score('Nacional', roc_matrix))\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[39m# print(\"false_positives: \",classifier.calculate_false_positives('Nacional', roc_matrix))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[79], line 150\u001b[0m, in \u001b[0;36mNewsClassifier.calculate_confusion_matrix\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m expected_result \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mcategoria\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    149\u001b[0m predicted_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_title(row[\u001b[39m'\u001b[39m\u001b[39mtitular\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 150\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfusion_matrix[expected_result][predicted_result] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk \n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"snowball_data\")\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class NewsClassifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.vocabulary = set()\n",
    "        self.vocabulary_size = None\n",
    "        self.word_occurencies_per_class = {}\n",
    "        self.total_tokens_per_class = defaultdict(int)\n",
    "        self.class_frequency_map = None\n",
    "        self.labels = None\n",
    "        self.confusion_matrix = None\n",
    "\n",
    "    def preprocess_text_spanish(self,text, use_stopwords, use_stemmer):\n",
    "        # Convert to lowercase and remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    \n",
    "        # Tokenization\n",
    "        tokens = text.split()\n",
    "        if use_stopwords:\n",
    "            # Remove stop words\n",
    "            stop_words = set(stopwords.words(\"spanish\"))\n",
    "            tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "        if use_stemmer:\n",
    "            # Stemming (use SnowballStemmer for Spanish)\n",
    "            stemmer = SnowballStemmer(\"spanish\")\n",
    "            tokens = [stemmer.stem(token) for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def split_dataset(self, test_percentage, dataset_path, categories):\n",
    "        file = dataset_path\n",
    "        news_df = pd.read_excel(file)\n",
    "        accepted_cats = categories\n",
    "        filtered_df = news_df[news_df['categoria'].isin(accepted_cats)]\n",
    "        \n",
    "        if test_percentage != 0:\n",
    "            self.train_df, self.test_df = train_test_split(filtered_df, test_size=test_percentage/100, random_state=42)\n",
    "            self.labels = self.train_df['categoria']\n",
    "        else:\n",
    "            self.train_df = filtered_df\n",
    "            self.labels = filtered_df['categoria']\n",
    "\n",
    "    def train(self, alpha=1):\n",
    "        matrix_per_category = {}\n",
    "        # Apply CountVectorizer to the titles per category\n",
    "        for label in self.labels.unique():\n",
    "            category_df = self.train_df[self.train_df['categoria'] == label]\n",
    "            preprocessed_titles = [self.preprocess_text_spanish(title, False, False) for title in category_df[\"titular\"]]\n",
    "            self.vocabulary = set(self.vocabulary).union(set([token for tokens in preprocessed_titles for token in tokens]))\n",
    "            preprocessed_titles_strings = [\" \".join(tokens) for tokens in preprocessed_titles]\n",
    "            vectorizer = CountVectorizer()\n",
    "            X = vectorizer.fit_transform(preprocessed_titles_strings)\n",
    "            count_matrix = X.toarray()\n",
    "            df = pd.DataFrame(count_matrix, columns=vectorizer.get_feature_names_out())\n",
    "            matrix_per_category[label] = df\n",
    "\n",
    "\n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "        self.class_frequency_map = {}\n",
    "\n",
    "        for label in self.labels.unique():\n",
    "            self.class_frequency_map[label] = (self.train_df['categoria'] == label).sum() / self.train_df.shape[0]\n",
    "\n",
    "        # Count word occurrences in each class\n",
    "        for label in self.labels.unique():\n",
    "            self.word_occurencies_per_class[label] = defaultdict(int)\n",
    "            for word in matrix_per_category[label].columns:\n",
    "                self.word_occurencies_per_class[label][word] = matrix_per_category[label][word].sum() \n",
    "            self.total_tokens_per_class[label] = matrix_per_category[label].sum().sum()     \n",
    "\n",
    "        self.conditional_probabilities = {}\n",
    "        # Calculate conditional probabilities for each word in each class\n",
    "        for label in self.labels.unique():\n",
    "            self.conditional_probabilities[label] = {}\n",
    "            for word in self.vocabulary:\n",
    "                self.conditional_probabilities[label][word] = (self.word_occurencies_per_class[label][word] + alpha) / (self.total_tokens_per_class[label] + alpha * self.total_tokens_per_class[label])\n",
    "            \n",
    "                \n",
    "    def predict_title(self, new_title):\n",
    "        # Preprocess the new title\n",
    "        new_title_tokens = self.preprocess_text_spanish(new_title, False, False)\n",
    "\n",
    "        # Calculate probabilities for each class\n",
    "        predicted_probabilities = {}\n",
    "        for class_label, class_frequency in self.class_frequency_map.items():\n",
    "            prob_word_given_class = class_frequency  # P(cat|token) = P(cat) * P(token|cat) = P(cat) * P(token1|cat) * P(token2|cat) * ... * P(tokenN|cat) \n",
    "            for token in new_title_tokens:\n",
    "                if token in self.vocabulary:\n",
    "                    prob_word_given_class *= self.conditional_probabilities[class_label][token]\n",
    "            predicted_probabilities[class_label] = prob_word_given_class\n",
    "\n",
    "        # Divide by the sum of all probabilities to get the normalized probability\n",
    "        total_probability = sum(predicted_probabilities.values())\n",
    "        for class_label in predicted_probabilities.keys():\n",
    "            predicted_probabilities[class_label] = predicted_probabilities[class_label] / total_probability\n",
    "        \n",
    "        print(max(predicted_probabilities, key=predicted_probabilities.get))\n",
    "        # Return the class with the highest probability\n",
    "        return max(predicted_probabilities, key=predicted_probabilities.get), predicted_probabilities\n",
    "       \n",
    "\n",
    "\n",
    "    def calculate_confusion_matrix_ROC(self,label):\n",
    "        roc_matrix = {}\n",
    "        roc_matrix[label] = {}\n",
    "        roc_matrix['other'] = {}\n",
    "        roc_matrix[label][label] =0\n",
    "        roc_matrix[label]['other'] =0\n",
    "        roc_matrix['other'][label] =0\n",
    "        roc_matrix['other']['other'] =0\n",
    "        for index, row in self.test_df.iterrows():\n",
    "            expected_result = row['categoria']\n",
    "            predicted_result = self.predict_title(row['titular'])\n",
    "            if expected_result==predicted_result:\n",
    "                if expected_result == label:\n",
    "                    roc_matrix[label][label] += 1\n",
    "                else:\n",
    "                   roc_matrix['other']['other'] += 1 \n",
    "            else:\n",
    "                if expected_result == label:\n",
    "                    roc_matrix[label]['other'] += 1\n",
    "                else:\n",
    "                    roc_matrix['other'][label] += 1\n",
    "        return roc_matrix\n",
    "    \n",
    "    \n",
    "    def calculate_confusion_matrix(self):\n",
    "        label_ocurrences = {}\n",
    "        self.confusion_matrix = {}\n",
    "        for label in self.class_frequency_map.keys():\n",
    "            label_ocurrences[label] = (self.test_df['categoria'] == label).sum()\n",
    "            self.confusion_matrix[label] = {}\n",
    "            for column_label in self.class_frequency_map.keys():\n",
    "                self.confusion_matrix[label][column_label] = 0\n",
    "        for index, row in self.test_df.iterrows():\n",
    "            expected_result = row['categoria']\n",
    "            predicted_result = self.predict_title(row['titular'])\n",
    "            self.confusion_matrix[expected_result][predicted_result] += 1\n",
    "    \n",
    "    def change_confusion_matrix_to_freq(self):\n",
    "        for row in self.confusion_matrix.keys():\n",
    "            for column in self.confusion_matrix[row].keys():\n",
    "                self.confusion_matrix[row][column] /= label_ocurrences[row]\n",
    "                \n",
    "    def print_confusion_matrix(self):\n",
    "        for row in self.confusion_matrix.keys():\n",
    "            for column in self.confusion_matrix[row].keys():\n",
    "                print(row, column, self.confusion_matrix[row][column])\n",
    "        for label in label_ocurrences.keys():\n",
    "            print(label,\":\", label_ocurrences[label])\n",
    "            \n",
    "    def calculate_accuracy(self, matrix):\n",
    "        asserted = 0\n",
    "        total = 0\n",
    "        for row in matrix.keys():\n",
    "            for column in matrix.keys():\n",
    "                if row == column:\n",
    "                    asserted += matrix[row][column]\n",
    "                total += matrix[row][column]\n",
    "        return asserted / total\n",
    "        \n",
    "    def calculate_true_positives(self, category, matrix):\n",
    "        false_negatives = 0\n",
    "        true_positives = 0\n",
    "        for column in matrix.keys():\n",
    "            if column != category:\n",
    "                false_negatives += matrix[category][column]\n",
    "            else:\n",
    "                true_positives += matrix[category][column]             \n",
    "        return true_positives/(true_positives + false_negatives)\n",
    "        \n",
    "    def calculate_precision(self, category, matrix):\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        for row in matrix.keys():\n",
    "            if row != category:\n",
    "                false_positives += matrix[row][category]\n",
    "            else:\n",
    "                true_positives += matrix[row][category]             \n",
    "        return true_positives/(true_positives + false_positives)\n",
    "        \n",
    "    def calculate_false_positives(self, category, matrix):\n",
    "        true_negatives = 0\n",
    "        false_positives = 0\n",
    "        for row in matrix.keys():\n",
    "            if row != category:\n",
    "                false_positives += matrix[row][category]\n",
    "                \n",
    "        for row in matrix.keys():\n",
    "            for column in matrix.keys():\n",
    "                if row == column and row != category:\n",
    "                    true_negatives += matrix[row][column]\n",
    "        return false_positives / (false_positives + true_negatives)\n",
    "        \n",
    "    def calculate_f1_score(self,category, matrix):\n",
    "        precision = self.calculate_precision(category, matrix)\n",
    "        recall = self.calculate_true_positives(category, matrix)\n",
    "        return (2*precision*recall)/(precision+recall)\n",
    "                \n",
    "\n",
    "file = 'inputs/NoticiasArgentinas.xlsx'\n",
    "categories = ['Nacional', \"Destacadas\", \"Entretenimiento\", \"Salud\"]\n",
    "classifier = NewsClassifier()\n",
    "classifier.split_dataset(20, file, categories) # 20% test, 80% train\n",
    "classifier.train()\n",
    "classifier.predict_title('Diabetes : este miércoles se realiza la jornada de prevención')\n",
    "classifier.calculate_confusion_matrix()\n",
    "classifier.print_confusion_matrix()\n",
    "# roc_matrix = classifier.calculate_confusion_matrix_ROC('Nacional')\n",
    "# print(\"accuracy: \",classifier.calculate_accuracy(roc_matrix))\n",
    "# print(\"true_positives: \",classifier.calculate_true_positives('Nacional',roc_matrix))\n",
    "# print(\"precision: \",classifier.calculate_precision('Nacional', roc_matrix))\n",
    "# print(\"f1-score: \",classifier.calculate_f1_score('Nacional', roc_matrix))\n",
    "# print(\"false_positives: \",classifier.calculate_false_positives('Nacional', roc_matrix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
