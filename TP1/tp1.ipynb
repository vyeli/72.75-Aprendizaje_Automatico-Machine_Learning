{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3ee81e-8c19-4bbe-adde-543dc38de9fe",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae43cf05-9951-43c0-a836-308fbb90f1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 0.2282357113077981\n",
      "E 0.7717642886922018\n",
      "\n",
      "I 0.623508361311908\n",
      "E 0.37649163868809205\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def train(self, df, alpha=1):\n",
    "        grouped_classes = df.groupby('Nacionalidad')\n",
    "        self.classes = df['Nacionalidad'].unique()\n",
    "        self.attributes = df.columns[0: -1]\n",
    "        self.class_probabilities = {}\n",
    "        self.attribute_probabilities = {}\n",
    "        total_len = len(df)\n",
    "\n",
    "        for _class, grouped in grouped_classes:\n",
    "            data_len = len(grouped)\n",
    "            self.class_probabilities[_class] = (data_len + alpha) / (total_len + alpha * len(self.classes))\n",
    "            self.attribute_probabilities[_class] = {}\n",
    "            for attribute in df.columns[0:-1]:    # Skipping last column (Nacionalidad)\n",
    "                self.attribute_probabilities[_class][attribute] = (grouped[attribute].sum() + alpha) / (data_len + alpha * len(self.attributes))\n",
    "    \n",
    "    def print_probabilities(self, values):\n",
    "        prediction = {}\n",
    "        for _class in self.classes:\n",
    "            probability = self.class_probabilities[_class]\n",
    "            for i,value in enumerate(values):\n",
    "                if value == 1:\n",
    "                    probability *= self.attribute_probabilities[_class][self.attributes[i]]\n",
    "                else: \n",
    "                    probability *= 1 - self.attribute_probabilities[_class][self.attributes[i]] # 1 - P(A) = P(not A)\n",
    "            prediction[_class] = probability\n",
    "        for _class, value in prediction.items():\n",
    "            print(_class, value / sum(prediction.values()))  # divide the probability by the sum of all probabilities to get the normalized probability\n",
    "\n",
    "\n",
    "file = 'inputs/PreferenciasBritanicos.xlsx'\n",
    "brits_df = pd.read_excel(file)\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(brits_df)\n",
    "# 1.B\n",
    "classifier.print_probabilities([1, 0, 1, 1, 0])\n",
    "print()\n",
    "# 1.C\n",
    "classifier.print_probabilities([0, 1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11587fba",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "## Paso 1: preprocesamiento del dataset\n",
    "Primero necesitamos hacer un preprocesamiento de los datos. Para esto decidimos reducir el dataset a 4 categorías, preferencialmente que no guarden demasiada correlación entre ellas en cuanto a las palabras utilizadas y asegurarnos que el modelo logre diferenciarlas correctamente sin tener que lidiar con las otras categorías. Una vez que el objetivo sea cumplido agregaremos devuelta las categorías filtradas para utilizar el dataset completo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48324d",
   "metadata": {},
   "source": [
    "Para el preprocesamiento de los titulos utilizaremos tecnicas utilizadas en NLP y las aplicaremos a los titulos, procedemos a la tokenización de las palabras en los titulos,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96e569ea-e095-4385-a562-4d6c74c5e26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Deportes': 3.379272523350326e-19, 'Nacional': 3.446076453877979e-20, 'Ciencia y Tecnologia': 0.03127512945500095, 'Salud': 5.673430315452938e-17}\n",
      "Predicted Category: Ciencia y Tecnologia\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class NewsClassifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.vocabulary = None\n",
    "        self.vocabulary_size = None\n",
    "        self.word_freq_per_class = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.total_tokens_per_class = defaultdict(int)\n",
    "        self.class_frequency_map = None\n",
    "        self.labels = None\n",
    "\n",
    "    def preprocess_text_spanish(self,text, use_stopwords, use_stemmer):\n",
    "        # Convert to lowercase and remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    \n",
    "        # Tokenization\n",
    "        tokens = text.split()\n",
    "        if use_stopwords:\n",
    "            # Remove stop words\n",
    "            stop_words = set(stopwords.words(\"spanish\"))\n",
    "            tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "        if use_stemmer:\n",
    "            # Stemming (use SnowballStemmer for Spanish)\n",
    "            stemmer = SnowballStemmer(\"spanish\")\n",
    "            tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "        return tokens\n",
    "\n",
    "    def split_dataset(self, test_percentage, dataset_path, categories):\n",
    "        file = dataset_path\n",
    "        news_df = pd.read_excel(file)\n",
    "        accepted_cats = categories\n",
    "        filtered_df = news_df[news_df['categoria'].isin(accepted_cats)]\n",
    "        if test_percentage != 0:\n",
    "            self.train_df, self.test_df = train_test_split(filtered_df, test_size=test_percentage/100, random_state=42)\n",
    "            self.labels = self.train_df['categoria']\n",
    "        else:\n",
    "            self.train_df = filtered_df\n",
    "            self.labels = filtered_df['categoria']\n",
    "\n",
    "    def train(self):\n",
    "        # a cada titular le aplico la función de tokenización\n",
    "        preprocessed_titles = [self.preprocess_text_spanish(title, True, True) for title in self.train_df['titular']]\n",
    "        preprocessed_titles_strings = [\" \".join(tokens) for tokens in preprocessed_titles]\n",
    "        vectorizer = CountVectorizer()\n",
    "        X = vectorizer.fit_transform(preprocessed_titles_strings)\n",
    "\n",
    "        # Build the vocabulary\n",
    "        self.vocabulary = vectorizer.get_feature_names_out()\n",
    "        self.vocabulary_size = len(vocabulary)\n",
    "\n",
    "        self.class_frequency_map = self.labels.value_counts(normalize=True).to_dict() #calcula la frecuencia relativa para cada clase\n",
    "\n",
    "        # Count word occurrences in each class\n",
    "        for i, doc in enumerate(X):\n",
    "            tokens = preprocessed_titles[i]\n",
    "            self.total_tokens_per_class[labels.iloc[i]] += len(tokens)\n",
    "            for word_idx in doc.indices:\n",
    "                word = self.vocabulary[word_idx]\n",
    "                for token in tokens:\n",
    "                    self.word_freq_per_class[token][self.labels.iloc[i]] += 1\n",
    "            \n",
    "\n",
    "        # Normalize word frequency per class by dividing by total tokens in each class\n",
    "        for word in self.word_freq_per_class:\n",
    "            for class_label in self.word_freq_per_class[word]:\n",
    "                self.word_freq_per_class[word][class_label] /= total_tokens_per_class[class_label]\n",
    "                \n",
    "    def predict_title(self, new_title):\n",
    "\n",
    "        # Preprocess the new title\n",
    "        new_title_tokens = self.preprocess_text_spanish(new_title, True, True)\n",
    "\n",
    "        # Calculate conditional probabilities\n",
    "        predicted_probabilities = {}\n",
    "        for class_label in self.class_frequency_map.keys():\n",
    "            class_frequency = self.class_frequency_map[class_label]\n",
    "            prob_word_given_class = class_frequency\n",
    "            for token in new_title_tokens:\n",
    "                word_freq = self.word_freq_per_class[token][class_label]\n",
    "                if word_freq == 0:\n",
    "                    # Applying Laplace smoothing\n",
    "                    smoothed_prob = 1 / (self.total_tokens_per_class[class_label] + self.vocabulary_size)\n",
    "                    prob_word_given_class *= smoothed_prob\n",
    "                else:\n",
    "                    prob_word_given_class *= word_freq / sum(self.word_freq_per_class[token].values())\n",
    "            predicted_probabilities[class_label] =prob_word_given_class\n",
    "\n",
    "        # Predict the category with the highest probability\n",
    "        print(predicted_probabilities)\n",
    "        predicted_category = max(predicted_probabilities, key=predicted_probabilities.get)\n",
    "        print(\"Predicted Category:\", predicted_category)\n",
    "\n",
    "file = 'inputs/NoticiasArgentinas.xlsx'\n",
    "categories = ['Nacional', 'Ciencia y Tecnologia', 'Deportes', 'Salud']\n",
    "classifier = NewsClassifier()\n",
    "classifier.split_dataset(20, file, categories)\n",
    "classifier.train()\n",
    "classifier.predict_title(\"Un 'huracán' de materia oscura se aproxima a la Tierra\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e9447c-d006-4ddb-b8e1-5e55e03e0ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b71fff58-b0d0-4c68-896d-adeac0ecaa2b",
   "metadata": {},
   "source": [
    "# Ejercicio 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff32a0d8-fcb9-464e-acdd-0011d0292a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    admit  gre  gpa  rank  probability\n",
      "0       0    0    0     1       0.0100\n",
      "1       0    0    0     2       0.0100\n",
      "2       0    0    0     3       0.0225\n",
      "3       0    0    0     4       0.0100\n",
      "4       0    0    1     1       0.0075\n",
      "5       0    0    1     2       0.0425\n",
      "6       0    0    1     3       0.0325\n",
      "7       0    0    1     4       0.0200\n",
      "8       0    1    0     1       0.0000\n",
      "9       0    1    0     2       0.0400\n",
      "10      0    1    0     3       0.0175\n",
      "11      0    1    0     4       0.0200\n",
      "12      0    1    1     1       0.0525\n",
      "13      0    1    1     2       0.1500\n",
      "14      0    1    1     3       0.1600\n",
      "15      0    1    1     4       0.0875\n",
      "16      1    0    0     1       0.0025\n",
      "17      1    0    0     2       0.0075\n",
      "18      1    0    0     3       0.0000\n",
      "19      1    0    0     4       0.0000\n",
      "20      1    0    1     1       0.0075\n",
      "21      1    0    1     2       0.0100\n",
      "22      1    0    1     3       0.0075\n",
      "23      1    0    1     4       0.0050\n",
      "24      1    1    0     1       0.0075\n",
      "25      1    1    0     2       0.0075\n",
      "26      1    1    0     3       0.0100\n",
      "27      1    1    0     4       0.0025\n",
      "28      1    1    1     1       0.0650\n",
      "29      1    1    1     2       0.1100\n",
      "30      1    1    1     3       0.0525\n",
      "31      1    1    1     4       0.0225\n",
      "3a:  0.41546427121512747\n",
      "3b:  0.19047619047619047\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## Preprocessing\n",
    "file = 'inputs/binary.csv'\n",
    "students_df = pd.read_csv(file)\n",
    "student_amount = len(students_df)\n",
    "\n",
    "# Creating auxiliar probabilites matrix\n",
    "aux_probabilities = []\n",
    "for admit in [0, 1]:\n",
    "    for gre in [0, 1]:\n",
    "        for gpa in [0, 1]:\n",
    "            for rank in [1, 2, 3, 4]:\n",
    "                aux_probabilities.append([admit, gre, gpa, rank, 0])\n",
    "                \n",
    "probs_df = pd.DataFrame(data=aux_probabilities, columns=['admit', 'gre', 'gpa', 'rank', 'probability'])\n",
    "\n",
    "for student_id in students_df.index:\n",
    "    admit = students_df['admit'][student_id]\n",
    "    rank = students_df['rank'][student_id]\n",
    "    discrete_gre = 1 if students_df['gre'][student_id] >= 500 else 0\n",
    "    discrete_gpa = 1 if students_df['gpa'][student_id] >= 3 else 0\n",
    "    probs_df.loc[(probs_df['admit'] == admit) & (probs_df['rank'] == rank) & (probs_df['gre'] == discrete_gre) & (probs_df['gpa'] == discrete_gpa), 'probability'] += 1 / student_amount\n",
    "\n",
    "print(probs_df)\n",
    "\n",
    "# 3.a -> P(!admitido | rango=1) = P(!admitido ^ rango=1) / P(rango=1)\n",
    "# P(!admitido ^ rango=1) = Sum_GPE(0,1){Sum_GPA(0,1){P(!admitido ^ GPE ^ GPA ^ rango=1}} = P(!admitido ^ !gpe ^ !gpa ^ rango = 1) * ... * P(!admitido ^ gpe ^ gpa ^ rango = 1)\n",
    "# Ej: P(!admitido ^ !gpe ^ !gpa ^ rango=1) = P(!admitido | !gpe ^ !gpa ^ rango=1) * P(!gpe | rango=1) * P(!gpa | rango=1) * P(rango=1)\n",
    "# p1: P(!admitido | !gpe ^ !gpa ^ rango=1)\n",
    "# p2: P(!gpe | rango=1)\n",
    "# p3: P(!gpa | rango=1)\n",
    "probability_3a = 0\n",
    "p_rango1 = sum(probs_df.loc[probs_df['rank'] == 1, 'probability'])\n",
    "\n",
    "for gre in [0, 1]:\n",
    "    for gpa in [0, 1]:\n",
    "        p1_parcial = probs_df.loc[(probs_df['admit'] == 0) & (probs_df['gre'] == gre) & (probs_df['gpa'] == gpa) & (probs_df['rank'] == 1), 'probability']\n",
    "        p1_tot = sum(probs_df.loc[(probs_df['gre'] == gre) & (probs_df['gpa'] == gpa) & (probs_df['rank'] == 1), 'probability'])\n",
    "        p1 = p1_parcial / p1_tot\n",
    "\n",
    "        p2_parcial = sum(probs_df.loc[(probs_df['gre'] == gre) & (probs_df['rank'] == 1), 'probability'])\n",
    "        p2 = p2_parcial / p_rango1\n",
    "\n",
    "        p3_parcial = sum(probs_df.loc[(probs_df['gpa'] == gpa) & (probs_df['rank'] == 1), 'probability'])\n",
    "        p3 = p3_parcial / p_rango1\n",
    "\n",
    "        probability_3a += float((p1 * p2 * p3).iloc[0])\n",
    "\n",
    "print('3a: ', probability_3a)\n",
    "\n",
    "# 3.b -> P(admitido | r=2 ^ !gre ^ gpa) = P(admitido ^ r=2 ^ !gre ^ gpa) / P(r=2 ^ !gre ^ gpa)\n",
    "p1 = probs_df.loc[(probs_df['admit'] == 1) & (probs_df['gre'] == 0) & (probs_df['gpa'] == 1) & (probs_df['rank'] == 2), 'probability']\n",
    "p2 = sum(probs_df.loc[(probs_df['gre'] == 0) & (probs_df['gpa'] == 1) & (probs_df['rank'] == 2), 'probability'])\n",
    "prob_3b = float((p1 / p2).iloc[0])\n",
    "print('3b: ', prob_3b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5c2f7-e35d-4bc9-a1bd-34167306412c",
   "metadata": {},
   "source": [
    "El proceso de aprendizaje es **paramétrico**, pues ya se conoce la estructura de condiciones. Por ende, no se necesita del algoritmo k2 para identificarla."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
