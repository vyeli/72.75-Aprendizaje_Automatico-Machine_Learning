{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3ee81e-8c19-4bbe-adde-543dc38de9fe",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae43cf05-9951-43c0-a836-308fbb90f1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 0.2282357113077981\n",
      "E 0.7717642886922018\n",
      "\n",
      "I 0.623508361311908\n",
      "E 0.37649163868809205\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def train(self, df, alpha=1):\n",
    "        grouped_classes = df.groupby('Nacionalidad')\n",
    "        self.classes = df['Nacionalidad'].unique()\n",
    "        self.attributes = df.columns[0: -1]\n",
    "        self.class_probabilities = {}\n",
    "        self.attribute_probabilities = {}\n",
    "        total_len = len(df)\n",
    "\n",
    "        for _class, grouped in grouped_classes:\n",
    "            data_len = len(grouped)\n",
    "            self.class_probabilities[_class] = (data_len + alpha) / (total_len + alpha * len(self.classes))\n",
    "            self.attribute_probabilities[_class] = {}\n",
    "            for attribute in df.columns[0:-1]:    # Skipping last column (Nacionalidad)\n",
    "                self.attribute_probabilities[_class][attribute] = (grouped[attribute].sum() + alpha) / (data_len + alpha * len(self.attributes))\n",
    "    \n",
    "    def print_probabilities(self, values):\n",
    "        prediction = {}\n",
    "        for _class in self.classes:\n",
    "            probability = self.class_probabilities[_class]\n",
    "            for i,value in enumerate(values):\n",
    "                if value == 1:\n",
    "                    probability *= self.attribute_probabilities[_class][self.attributes[i]]\n",
    "                else: \n",
    "                    probability *= 1 - self.attribute_probabilities[_class][self.attributes[i]] # 1 - P(A) = P(not A)\n",
    "            prediction[_class] = probability\n",
    "        for _class, value in prediction.items():\n",
    "            print(_class, value / sum(prediction.values()))  # divide the probability by the sum of all probabilities to get the normalized probability\n",
    "\n",
    "\n",
    "file = 'inputs/PreferenciasBritanicos.xlsx'\n",
    "brits_df = pd.read_excel(file)\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(brits_df)\n",
    "# 1.B\n",
    "classifier.print_probabilities([1, 0, 1, 1, 0])\n",
    "print()\n",
    "# 1.C\n",
    "classifier.print_probabilities([0, 1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11587fba",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "## Paso 1: preprocesamiento del dataset\n",
    "Primero necesitamos hacer un preprocesamiento de los datos. Para esto decidimos reducir el dataset a 4 categorías, preferencialmente que no guarden demasiada correlación entre ellas en cuanto a las palabras utilizadas y asegurarnos que el modelo logre diferenciarlas correctamente sin tener que lidiar con las otras categorías. Una vez que el objetivo sea cumplido agregaremos devuelta las categorías filtradas para utilizar el dataset completo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48324d",
   "metadata": {},
   "source": [
    "Para el preprocesamiento de los titulos utilizaremos tecnicas utilizadas en NLP y las aplicaremos a los titulos, procedemos a la tokenización de las palabras en los titulos,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "96e569ea-e095-4385-a562-4d6c74c5e26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nacional': 9.797700858272893e-27, 'Ciencia y Tecnologia': 3.5620939946273e-27, 'Deportes': 3.8314053918028403e-16, 'Salud': 3.3577926800814774e-28}\n",
      "Predicted Category: Deportes\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def preprocess_text_spanish(text):\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"spanish\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming (use SnowballStemmer for Spanish)\n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "file = 'inputs/NoticiasArgentinas.xlsx'\n",
    "news_df = pd.read_excel(file)\n",
    "accepted_cats = ['Nacional', 'Deportes', 'Salud', 'Ciencia y Tecnologia']\n",
    "filtered_df = news_df[news_df['categoria'].isin(accepted_cats)]\n",
    "labels = filtered_df['categoria']\n",
    "\n",
    "preprocessed_titles = [preprocess_text_spanish(title) for title in filtered_df['titular']] # a cada titular le aplico la función\n",
    "preprocessed_titles_strings = [\" \".join(tokens) for tokens in preprocessed_titles]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_titles_strings)\n",
    "\n",
    "# Build the vocabulary\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "class_frequency_map = labels.value_counts(normalize=True).to_dict() #calcula la frecuencia relativa para cada clase\n",
    "word_freq_per_class = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "total_tokens_per_class = defaultdict(int)\n",
    "\n",
    "# Count word occurrences in each class\n",
    "for i, doc in enumerate(X):\n",
    "    tokens = preprocessed_titles[i]\n",
    "    total_tokens_per_class[labels.iloc[i]] += len(tokens)\n",
    "    for word_idx in doc.indices:\n",
    "        word = vocabulary[word_idx]\n",
    "        for token in tokens:\n",
    "            word_freq_per_class[token][labels.iloc[i]] += 1\n",
    "\n",
    "# Example new title to classify\n",
    "new_title = \"Paulo Gazzaniga, el arquero sorpresa de Lionel Scaloni\"\n",
    "\n",
    "# Preprocess the new title\n",
    "new_title_tokens = preprocess_text_spanish(new_title)\n",
    "\n",
    "# Calculate conditional probabilities\n",
    "predicted_probabilities = {}\n",
    "for class_label in class_frequency_map.keys():\n",
    "    class_frequency = class_frequency_map[class_label]\n",
    "    prob_word_given_class = 1.0\n",
    "    for token in new_title_tokens:\n",
    "        word_freq = word_freq_per_class[token][class_label]\n",
    "        if word_freq==0:\n",
    "        # Applying Laplace smoothing\n",
    "            prob_word_given_class *= (word_freq + 1) / (total_tokens_per_class[class_label] + vocabulary_size)\n",
    "        else:\n",
    "             prob_word_given_class*= word_freq / total_tokens_per_class[class_label]\n",
    "    predicted_probabilities[class_label] = class_frequency *  prob_word_given_class\n",
    "\n",
    "# Predict the category with the highest probability\n",
    "print(predicted_probabilities)\n",
    "predicted_category = max(predicted_probabilities, key=predicted_probabilities.get)\n",
    "print(\"Predicted Category:\", predicted_category)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e9447c-d006-4ddb-b8e1-5e55e03e0ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b71fff58-b0d0-4c68-896d-adeac0ecaa2b",
   "metadata": {},
   "source": [
    "# Ejercicio 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff32a0d8-fcb9-464e-acdd-0011d0292a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3a:  0.41546427121512747\n",
      "3b:  0.19047619047619047\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## Preprocessing\n",
    "file = 'inputs/binary.csv'\n",
    "students_df = pd.read_csv(file)\n",
    "student_amount = len(students_df)\n",
    "\n",
    "# Creating auxiliar probabilites matrix\n",
    "aux_probabilities = []\n",
    "for admit in [0, 1]:\n",
    "    for gre in [0, 1]:\n",
    "        for gpa in [0, 1]:\n",
    "            for rank in [1, 2, 3, 4]:\n",
    "                aux_probabilities.append([admit, gre, gpa, rank, 0])\n",
    "                \n",
    "probs_df = pd.DataFrame(data=aux_probabilities, columns=['admit', 'gre', 'gpa', 'rank', 'probability'])\n",
    "\n",
    "for student_id in students_df.index:\n",
    "    admit = students_df['admit'][student_id]\n",
    "    rank = students_df['rank'][student_id]\n",
    "    discrete_gre = 1 if students_df['gre'][student_id] >= 500 else 0\n",
    "    discrete_gpa = 1 if students_df['gpa'][student_id] >= 3 else 0\n",
    "    probs_df.loc[(probs_df['admit'] == admit) & (probs_df['rank'] == rank) & (probs_df['gre'] == discrete_gre) & (probs_df['gpa'] == discrete_gpa), 'probability'] += 1 / student_amount\n",
    "\n",
    "# 3.a -> P(!admitido | rango=1) = P(!admitido ^ rango=1) / P(rango=1)\n",
    "# P(!admitido ^ rango=1) = Sum_GPE(0,1){Sum_GPA(0,1){P(!admitido ^ GPE ^ GPA ^ rango=1}} = P(!admitido ^ !gpe ^ !gpa ^ rango = 1) * ... * P(!admitido ^ gpe ^ gpa ^ rango = 1)\n",
    "# Ej: P(!admitido ^ !gpe ^ !gpa ^ rango=1) = P(!admitido | !gpe ^ !gpa ^ rango=1) * P(!gpe | rango=1) * P(!gpa | rango=1) * P(rango=1)\n",
    "# p1: P(!admitido | !gpe ^ !gpa ^ rango=1)\n",
    "# p2: P(!gpe | rango=1)\n",
    "# p3: P(!gpa | rango=1)\n",
    "probability_3a = 0\n",
    "p_rango1 = sum(probs_df.loc[probs_df['rank'] == 1, 'probability'])\n",
    "\n",
    "for gre in [0, 1]:\n",
    "    for gpa in [0, 1]:\n",
    "        p1_parcial = probs_df.loc[(probs_df['admit'] == 0) & (probs_df['gre'] == gre) & (probs_df['gpa'] == gpa) & (probs_df['rank'] == 1), 'probability']\n",
    "        p1_tot = sum(probs_df.loc[(probs_df['gre'] == gre) & (probs_df['gpa'] == gpa) & (probs_df['rank'] == 1), 'probability'])\n",
    "        p1 = p1_parcial / p1_tot\n",
    "\n",
    "        p2_parcial = sum(probs_df.loc[(probs_df['gre'] == gre) & (probs_df['rank'] == 1), 'probability'])\n",
    "        p2 = p2_parcial / p_rango1\n",
    "\n",
    "        p3_parcial = sum(probs_df.loc[(probs_df['gpa'] == gpa) & (probs_df['rank'] == 1), 'probability'])\n",
    "        p3 = p3_parcial / p_rango1\n",
    "\n",
    "        probability_3a += float((p1 * p2 * p3).iloc[0])\n",
    "\n",
    "print('3a: ', probability_3a)\n",
    "\n",
    "# 3.b -> P(admitido | r=2 ^ !gre ^ gpa) = P(admitido ^ r=2 ^ !gre ^ gpa) / P(r=2 ^ !gre ^ gpa)\n",
    "p1 = probs_df.loc[(probs_df['admit'] == 1) & (probs_df['gre'] == 0) & (probs_df['gpa'] == 1) & (probs_df['rank'] == 2), 'probability']\n",
    "p2 = sum(probs_df.loc[(probs_df['gre'] == 0) & (probs_df['gpa'] == 1) & (probs_df['rank'] == 2), 'probability'])\n",
    "prob_3b = float((p1 / p2).iloc[0])\n",
    "print('3b: ', prob_3b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5c2f7-e35d-4bc9-a1bd-34167306412c",
   "metadata": {},
   "source": [
    "El proceso de aprendizaje es **paramétrico**, pues ya se conoce la estructura de condiciones. Por ende, no se necesita del algoritmo k2 para identificarla."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
