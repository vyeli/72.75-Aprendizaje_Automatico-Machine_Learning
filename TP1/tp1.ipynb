{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3ee81e-8c19-4bbe-adde-543dc38de9fe",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae43cf05-9951-43c0-a836-308fbb90f1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 0.25260461382104393\n",
      "E 0.7473953861789561\n",
      "\n",
      "I 0.623508361311908\n",
      "E 0.37649163868809205\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def train(self, df, alpha=1):\n",
    "        grouped_classes = df.groupby('Nacionalidad')\n",
    "        self.classes = df['Nacionalidad'].unique()\n",
    "        self.attributes = df.columns[0: -1]\n",
    "        self.class_probabilities = {}\n",
    "        self.attribute_probabilities = {}\n",
    "        total_len = len(df)\n",
    "\n",
    "        for _class, grouped in grouped_classes:\n",
    "            data_len = len(grouped)\n",
    "            self.class_probabilities[_class] = (data_len + alpha) / (total_len + alpha * len(self.classes))\n",
    "            self.attribute_probabilities[_class] = {}\n",
    "            for attribute in df.columns[0:-1]:    # Skipping last column (Nacionalidad)\n",
    "                self.attribute_probabilities[_class][attribute] = (grouped[attribute].sum() + alpha) / (data_len + alpha * len(self.attributes))\n",
    "    \n",
    "    def print_probabilities(self, values):\n",
    "        prediction = {}\n",
    "        for _class in self.classes:\n",
    "            probability = self.class_probabilities[_class]\n",
    "            for i,value in enumerate(values):\n",
    "                if value == 1:\n",
    "                    probability *= self.attribute_probabilities[_class][self.attributes[i]]\n",
    "                else: \n",
    "                    probability *= 1 - self.attribute_probabilities[_class][self.attributes[i]] # 1 - P(A) = P(not A)\n",
    "            prediction[_class] = probability\n",
    "        for _class, value in prediction.items():\n",
    "            print(_class, value / sum(prediction.values()))  # divide the probability by the sum of all probabilities to get the normalized probability\n",
    "\n",
    "\n",
    "file = 'inputs/PreferenciasBritanicos.xlsx'\n",
    "brits_df = pd.read_excel(file)\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(brits_df)\n",
    "# 1.B\n",
    "classifier.print_probabilities([1, 0, 1, 1, 1])\n",
    "print()\n",
    "# 1.C\n",
    "classifier.print_probabilities([0, 1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11587fba",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "## Paso 1: preprocesamiento del dataset\n",
    "Primero necesitamos hacer un preprocesamiento de los datos. Para esto decidimos reducir el dataset a 4 categorías, preferencialmente que no guarden demasiada correlación entre ellas en cuanto a las palabras utilizadas y asegurarnos que el modelo logre diferenciarlas correctamente sin tener que lidiar con las otras categorías. Una vez que el objetivo sea cumplido agregaremos devuelta las categorías filtradas para utilizar el dataset completo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48324d",
   "metadata": {},
   "source": [
    "Para el preprocesamiento de los titulos utilizaremos tecnicas utilizadas en NLP y las aplicaremos a los titulos, procedemos a la tokenización de las palabras en los titulos,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96e569ea-e095-4385-a562-4d6c74c5e26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7362958157638664\n",
      "true_positives:  0.6763540290620872\n",
      "precision:  0.4740740740740741\n",
      "f1-score:  0.5574305933587371\n",
      "false_positives:  0.24419604471195186\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class NewsClassifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.vocabulary = None\n",
    "        self.vocabulary_size = None\n",
    "        self.word_freq_per_class = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.total_tokens_per_class = defaultdict(int)\n",
    "        self.class_frequency_map = None\n",
    "        self.labels = None\n",
    "        self.confusion_matrix = None\n",
    "\n",
    "    def preprocess_text_spanish(self,text, use_stopwords, use_stemmer):\n",
    "        # Convert to lowercase and remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    \n",
    "        # Tokenization\n",
    "        tokens = text.split()\n",
    "        if use_stopwords:\n",
    "            # Remove stop words\n",
    "            stop_words = set(stopwords.words(\"spanish\"))\n",
    "            tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "        if use_stemmer:\n",
    "            # Stemming (use SnowballStemmer for Spanish)\n",
    "            stemmer = SnowballStemmer(\"spanish\")\n",
    "            tokens = [stemmer.stem(token) for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def split_dataset(self, test_percentage, dataset_path, categories):\n",
    "        file = dataset_path\n",
    "        news_df = pd.read_excel(file)\n",
    "        accepted_cats = categories\n",
    "        filtered_df = news_df[news_df['categoria'].isin(accepted_cats)]\n",
    "        if test_percentage != 0:\n",
    "            self.train_df, self.test_df = train_test_split(filtered_df, test_size=test_percentage/100, random_state=42)\n",
    "            self.labels = self.train_df['categoria']\n",
    "        else:\n",
    "            self.train_df = filtered_df\n",
    "            self.labels = filtered_df['categoria']\n",
    "\n",
    "    def train(self):\n",
    "        # a cada titular le aplico la función de tokenización\n",
    "        preprocessed_titles = [self.preprocess_text_spanish(title, True, True) for title in self.train_df['titular']]\n",
    "        preprocessed_titles_strings = [\" \".join(tokens) for tokens in preprocessed_titles]\n",
    "        vectorizer = CountVectorizer()\n",
    "        X = vectorizer.fit_transform(preprocessed_titles_strings)\n",
    "\n",
    "        # Build the vocabulary\n",
    "        self.vocabulary = vectorizer.get_feature_names_out()\n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "\n",
    "        self.class_frequency_map = self.labels.value_counts(normalize=True).to_dict() #calcula la frecuencia relativa para cada clase\n",
    "\n",
    "        # Count word occurrences in each class\n",
    "        for i, doc in enumerate(X):\n",
    "            tokens = preprocessed_titles[i]\n",
    "            self.total_tokens_per_class[self.labels.iloc[i]] += len(tokens)\n",
    "            for word_idx in doc.indices:\n",
    "                word = self.vocabulary[word_idx]\n",
    "                for token in tokens:\n",
    "                    self.word_freq_per_class[token][self.labels.iloc[i]] += 1\n",
    "        \n",
    "                \n",
    "    def predict_title(self, new_title):\n",
    "\n",
    "        # Preprocess the new title\n",
    "        new_title_tokens = self.preprocess_text_spanish(new_title, False, False)\n",
    "\n",
    "        # Calculate conditional probabilities\n",
    "        predicted_probabilities = {}\n",
    "        for class_label in self.class_frequency_map.keys():\n",
    "            class_frequency = self.class_frequency_map[class_label]\n",
    "            prob_word_given_class = class_frequency\n",
    "            for token in new_title_tokens:\n",
    "                word_freq = self.word_freq_per_class[token][class_label]\n",
    "                prob_word_given_class *= (word_freq+1) / (sum(self.word_freq_per_class[token].values()) + len(self.class_frequency_map.keys()))\n",
    "            predicted_probabilities[class_label] =prob_word_given_class\n",
    "\n",
    "        # Predict the category with the highest probability\n",
    "        #print(predicted_probabilities)\n",
    "        predicted_category = max(predicted_probabilities, key=predicted_probabilities.get)\n",
    "        #print(\"Predicted Category:\", predicted_category)\n",
    "        return predicted_category\n",
    "\n",
    "    def calculate_confusion_matrix_ROC(self,label):\n",
    "        roc_matrix = {}\n",
    "        roc_matrix[label] = {}\n",
    "        roc_matrix['other'] = {}\n",
    "        roc_matrix[label][label] =0\n",
    "        roc_matrix[label]['other'] =0\n",
    "        roc_matrix['other'][label] =0\n",
    "        roc_matrix['other']['other'] =0\n",
    "        for index, row in self.test_df.iterrows():\n",
    "            expected_result = row['categoria']\n",
    "            predicted_result = self.predict_title(row['titular'])\n",
    "            if expected_result==predicted_result:\n",
    "                if expected_result == label:\n",
    "                    roc_matrix[label][label] += 1\n",
    "                else:\n",
    "                   roc_matrix['other']['other'] += 1 \n",
    "            else:\n",
    "                if expected_result == label:\n",
    "                    roc_matrix[label]['other'] += 1\n",
    "                else:\n",
    "                    roc_matrix['other'][label] += 1\n",
    "        return roc_matrix\n",
    "    \n",
    "    \n",
    "    def calculate_confusion_matrix(self):\n",
    "        label_ocurrences = {}\n",
    "        self.confusion_matrix = {}\n",
    "        for label in self.class_frequency_map.keys():\n",
    "            label_ocurrences[label] = (self.test_df['categoria'] == label).sum()\n",
    "            self.confusion_matrix[label] = {}\n",
    "            for column_label in self.class_frequency_map.keys():\n",
    "                self.confusion_matrix[label][column_label] = 0\n",
    "        for index, row in self.test_df.iterrows():\n",
    "            expected_result = row['categoria']\n",
    "            predicted_result = self.predict_title(row['titular'])\n",
    "            self.confusion_matrix[expected_result][predicted_result] += 1\n",
    "    \n",
    "    def change_confusion_matrix_to_freq(self):\n",
    "        for row in self.confusion_matrix.keys():\n",
    "            for column in self.confusion_matrix[row].keys():\n",
    "                self.confusion_matrix[row][column] /= label_ocurrences[row]\n",
    "                \n",
    "    def print_confusion_matrix(self):\n",
    "        for row in self.confusion_matrix.keys():\n",
    "            for column in self.confusion_matrix[row].keys():\n",
    "                print(row, column, self.confusion_matrix[row][column])\n",
    "        for label in label_ocurrences.keys():\n",
    "            print(label,\":\", label_ocurrences[label])\n",
    "            \n",
    "    def calculate_accuracy(self, matrix):\n",
    "        asserted = 0\n",
    "        total = 0\n",
    "        for row in matrix.keys():\n",
    "            for column in matrix.keys():\n",
    "                if row == column:\n",
    "                    asserted += matrix[row][column]\n",
    "                total += matrix[row][column]\n",
    "        return asserted / total\n",
    "        \n",
    "    def calculate_true_positives(self, category, matrix):\n",
    "        false_negatives = 0\n",
    "        true_positives = 0\n",
    "        for column in matrix.keys():\n",
    "            if column != category:\n",
    "                false_negatives += matrix[category][column]\n",
    "            else:\n",
    "                true_positives += matrix[category][column]             \n",
    "        return true_positives/(true_positives + false_negatives)\n",
    "        \n",
    "    def calculate_precision(self, category, matrix):\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        for row in matrix.keys():\n",
    "            if row != category:\n",
    "                false_positives += matrix[row][category]\n",
    "            else:\n",
    "                true_positives += matrix[row][category]             \n",
    "        return true_positives/(true_positives + false_positives)\n",
    "        \n",
    "    def calculate_false_positives(self, category, matrix):\n",
    "        true_negatives = 0\n",
    "        false_positives = 0\n",
    "        for row in matrix.keys():\n",
    "            if row != category:\n",
    "                false_positives += matrix[row][category]\n",
    "                \n",
    "        for row in matrix.keys():\n",
    "            for column in matrix.keys():\n",
    "                if row == column and row != category:\n",
    "                    true_negatives += matrix[row][column]\n",
    "        return false_positives / (false_positives + true_negatives)\n",
    "        \n",
    "    def calculate_f1_score(self,category, matrix):\n",
    "        precision = self.calculate_precision(category, matrix)\n",
    "        recall = self.calculate_true_positives(category, matrix)\n",
    "        return (2*precision*recall)/(precision+recall)\n",
    "                \n",
    "\n",
    "file = 'inputs/NoticiasArgentinas.xlsx'\n",
    "categories = ['Nacional', 'Ciencia y Tecnologia', 'Deportes', 'Salud']\n",
    "classifier = NewsClassifier()\n",
    "classifier.split_dataset(20, file, categories)\n",
    "classifier.train()\n",
    "classifier.predict_title('La próxima semana habrá paro total de transporte')\n",
    "roc_matrix = classifier.calculate_confusion_matrix_ROC('Nacional')\n",
    "print(\"accuracy: \",classifier.calculate_accuracy(roc_matrix))\n",
    "print(\"true_positives: \",classifier.calculate_true_positives('Nacional',roc_matrix))\n",
    "print(\"precision: \",classifier.calculate_precision('Nacional', roc_matrix))\n",
    "print(\"f1-score: \",classifier.calculate_f1_score('Nacional', roc_matrix))\n",
    "print(\"false_positives: \",classifier.calculate_false_positives('Nacional', roc_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e9447c-d006-4ddb-b8e1-5e55e03e0ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727f97b-db68-4c48-b83e-c7f19d3c2674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
